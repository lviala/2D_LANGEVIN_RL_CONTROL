{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from tensorforce.agents import Agent\n",
    "from tensorforce.environments import Environment\n",
    "from tensorforce.execution import Runner\n",
    "\n",
    "from LANGEVIN2D_ENV import Langevin2D_Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saver directory\n",
    "directory = os.path.join(os.getcwd(), 'agents' ,'saver_data_D_1em4_dta_0p01_maxa_1_ep100_lstm2_64_gr_1_wn_1_r_m_e_0p1')\n",
    "\n",
    "# Environment Parameters\n",
    "env_params = {\n",
    "    \"dt\": 0.0005,\n",
    "    \"T\" : 100.0,\n",
    "    \"a\" : 10.0 +10.0j,\n",
    "    \"b\" : -5.0e2,\n",
    "    \"D\" : 1.0e-4,\n",
    "    \"x0\": 0.03 + 0.0j\n",
    "    }\n",
    "\n",
    "# Controller Parameters\n",
    "optimization_params = {\n",
    "    \"min_value_forcing\": -1.0,\n",
    "    \"max_value_forcing\": 1.0\n",
    "    }\n",
    "\n",
    "# Training Parameters\n",
    "training_params = {\n",
    "    \"num_episodes\" : 100,\n",
    "    \"dt_action\"    : 0.01\n",
    "}\n",
    "\n",
    "# Compute environment and action input timesteps\n",
    "n_env_steps = int(training_params[\"dt_action\"] / env_params[\"dt\"])\n",
    "max_episode_timesteps = int(env_params[\"T\"]/env_params[\"dt\"]/n_env_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "200000\n"
    }
   ],
   "source": [
    "# Create and instance of the complex Stuart-Landau environment\n",
    "environment = Langevin2D_Env(n_env_steps = n_env_steps)\n",
    "environment.env_params = env_params\n",
    "environment.optimization_params = optimization_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify network architecture - 2 layers/64 neurons\n",
    "policy_network = [\n",
    "    [   \n",
    "        dict(type='retrieve', tensors='observation'),\n",
    "        dict(type='dense', size=32),\n",
    "        dict(type='dense', size=32),\n",
    "        dict(type='register' , tensor ='intermed-1')\n",
    "    ],\n",
    "    [   \n",
    "        dict(type='retrieve', tensors='prev_action'),\n",
    "        dict(type='dense', size=32),\n",
    "        dict(type='dense', size=32),\n",
    "        dict(type='register' , tensor ='intermed-2')\n",
    "    ],\n",
    "    [\n",
    "        dict(type='retrieve', tensors=['intermed-1','intermed-2'], aggregation='concat'),\n",
    "        dict(type='internal_lstm', size=64, length=1, bias=True),\n",
    "        dict(type='internal_lstm', size=64, length=1, bias=True),\n",
    "        dict(type='dense', size=16),\n",
    "    ]\n",
    "]\n",
    "\n",
    "network = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Create CheckpointSaverHook.\nWARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /Users/lucienviala/Documents/IMPERIAL/PROJECT/CODE/2D_LANGEVIN_CONTROL_OBS_ACT/agents/saver_data_D_1em4_dta_0p01_maxa_1_ep100_lstm2_64_gr_1_wn_1_r_m_e_0p1/agent-0\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Saving checkpoints for 0 into /Users/lucienviala/Documents/IMPERIAL/PROJECT/CODE/2D_LANGEVIN_CONTROL_OBS_ACT/agents/saver_data_D_1em4_dta_0p01_maxa_1_ep100_lstm2_64_gr_1_wn_1_r_m_e_0p1/agent.\n"
    }
   ],
   "source": [
    "# Specify the agent parameters - PPO algorithm\n",
    "agent = Agent.create(\n",
    "    # Agent + Environment\n",
    "    agent='ppo',  # Agent specification\n",
    "    environment=environment,  # Environment object\n",
    "    exploration=0.1,\n",
    "    # Network\n",
    "    network=policy_network,  # Policy NN specification\n",
    "    # Optimization\n",
    "    batch_size=1,  # Number of episodes per update batch\n",
    "    learning_rate=1e-2,  # Optimizer learning rate\n",
    "    subsampling_fraction=0.75,  # Fraction of batch timesteps to subsample\n",
    "    optimization_steps=25,\n",
    "    # Reward estimation\n",
    "    likelihood_ratio_clipping=0.2, # The epsilon of the ppo CLI objective\n",
    "    estimate_terminal=False,  # Whether to estimate the value of terminal states\n",
    "    # TODO: gae_lambda=0.97 doesn't currently exist - ???\n",
    "    # Critic\n",
    "    critic_network=network,  # Critic NN specification\n",
    "    critic_optimizer=dict(\n",
    "        type='multi_step', num_steps=5,\n",
    "        optimizer=dict(type='adam', learning_rate=1e-2)\n",
    "    ),\n",
    "    # Regularization\n",
    "    entropy_regularization=0.01,  # To discourage policy from being too 'certain'\n",
    "    # TensorFlow\n",
    "    saver=dict(directory=directory),  # TensorFlow saver configuration for periodic implicit saving\n",
    "    # TensorBoard Summarizer\n",
    "    summarizer=dict(directory=os.path.join(directory, 'summarizer') , labels=\"all\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "20 10000\n"
    }
   ],
   "source": [
    "# Set up control time with reference to simulation time\n",
    "dt_action = 0.01\n",
    "dt = environment.env_params[\"dt\"]\n",
    "T = environment.env_params[\"T\"]\n",
    "n_env_steps = int(dt_action / dt)\n",
    "n_actions = int(T/dt/n_env_steps)\n",
    "print(n_env_steps,n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'observation': array([-0.056704  ,  0.06827483]), 'prev_action': array([0., 0.])} <class 'dict'>\n"
    }
   ],
   "source": [
    "# Initiate environment to initial state\n",
    "time  = np.zeros((environment.max_episode_timesteps()))\n",
    "state = environment.reset()\n",
    "\n",
    "# Episode reward - defined as magnitude of the complex state\n",
    "sum_rewards = 0.0\n",
    "\n",
    "# Initialize agent internals for agents with internal RNNs\n",
    "internals = agent.initial_internals()\n",
    "\n",
    "print(state , type(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "OrderedDict([('observation', {'type': 'float', 'shape': (2,)}),\n             ('prev_action', {'type': 'float', 'shape': (2,)})])"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "agent.states_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "action , internals = agent.act(states=state, internals=internals, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-0.00605154  0.00741863]\n"
    }
   ],
   "source": [
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitvenvvenv7f748138af854e409b8185cb62dad0fb",
   "display_name": "Python 3.7.4 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}