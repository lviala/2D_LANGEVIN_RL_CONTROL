{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from tensorforce.agents import Agent\n",
    "from tensorforce.environments import Environment\n",
    "from tensorforce.execution import Runner\n",
    "\n",
    "from LANGEVIN2D_ENV import Langevin2D_Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saver directory\n",
    "directory = os.path.join(os.getcwd(), 'agents' ,'saver_data_D_1em4_dta_0p01_maxa_1_ep100_lstm2_64_gr_1_wn_1_r_m_e_0p1')\n",
    "\n",
    "# Environment Parameters\n",
    "env_params = {\n",
    "    \"dt\": 0.0005,\n",
    "    \"T\" : 100.0,\n",
    "    \"a\" : 10.0 +10.0j,\n",
    "    \"b\" : -5.0e2,\n",
    "    \"D\" : 1.0e-4,\n",
    "    \"x0\": 0.03 + 0.0j\n",
    "    }\n",
    "\n",
    "# Controller Parameters\n",
    "optimization_params = {\n",
    "    \"min_value_forcing\": -1.0,\n",
    "    \"max_value_forcing\": 1.0\n",
    "    }\n",
    "\n",
    "# Training Parameters\n",
    "training_params = {\n",
    "    \"num_episodes\" : 100,\n",
    "    \"dt_action\"    : 0.01\n",
    "}\n",
    "\n",
    "# Compute environment and action input timesteps\n",
    "n_env_steps = int(training_params[\"dt_action\"] / env_params[\"dt\"])\n",
    "max_episode_timesteps = int(env_params[\"T\"]/env_params[\"dt\"]/n_env_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "200000\n"
    }
   ],
   "source": [
    "# Create and instance of the complex Stuart-Landau environment\n",
    "environment = Langevin2D_Env(n_env_steps = n_env_steps)\n",
    "environment.env_params = env_params\n",
    "environment.optimization_params = optimization_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify network architecture - 2 layers/64 neurons\n",
    "policy_network = \"auto\"\n",
    "\n",
    "network = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\n"
    }
   ],
   "source": [
    "# Specify the agent parameters - PPO algorithm\n",
    "agent = Agent.create(\n",
    "    # Agent + Environment\n",
    "    agent='ppo',  # Agent specification\n",
    "    environment=environment,  # Environment object\n",
    "    exploration=0.1,\n",
    "    # Network\n",
    "    network=policy_network,  # Policy NN specification\n",
    "    # Optimization\n",
    "    batch_size=1,  # Number of episodes per update batch\n",
    "    learning_rate=1e-2,  # Optimizer learning rate\n",
    "    subsampling_fraction=0.75,  # Fraction of batch timesteps to subsample\n",
    "    optimization_steps=25,\n",
    "    # Reward estimation\n",
    "    likelihood_ratio_clipping=0.2, # The epsilon of the ppo CLI objective\n",
    "    estimate_terminal=False,  # Whether to estimate the value of terminal states\n",
    "    # TODO: gae_lambda=0.97 doesn't currently exist - ???\n",
    "    # Critic\n",
    "    critic_network=network,  # Critic NN specification\n",
    "    critic_optimizer=dict(\n",
    "        type='multi_step', num_steps=5,\n",
    "        optimizer=dict(type='adam', learning_rate=1e-2)\n",
    "    ),\n",
    "    # Regularization\n",
    "    entropy_regularization=0.01,  # To discourage policy from being too 'certain'\n",
    "    # TensorFlow\n",
    "    #saver=dict(directory=directory),  # TensorFlow saver configuration for periodic implicit saving\n",
    "    # TensorBoard Summarizer\n",
    "    #summarizer=dict(directory=os.path.join(directory, 'summarizer') , labels=\"all\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "20 10000\n"
    }
   ],
   "source": [
    "# Set up control time with reference to simulation time\n",
    "dt_action = 0.01\n",
    "dt = environment.env_params[\"dt\"]\n",
    "T = environment.env_params[\"T\"]\n",
    "n_env_steps = int(dt_action / dt)\n",
    "n_actions = int(T/dt/n_env_steps)\n",
    "print(n_env_steps,n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runner definition - Serial runner\n",
    "runner = Runner(\n",
    "    environment=environment,\n",
    "    agent=agent,\n",
    "    max_episode_timesteps=2,\n",
    "    #evaluation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Episodes:   0%|          | 0/3 [00:00, reward=0.00, ts/ep=0, sec/ep=0.00, ms/ts=0.0, agent=0.0%](0.03+0j)\nEpisodes:  33%|███▎      | 1/3 [00:08, reward=-0.12, ts/ep=2, sec/ep=8.62, ms/ts=4312.2, agent=100.0%](0.03+0j)\nEpisodes:  67%|██████▋   | 2/3 [00:13, reward=-0.10, ts/ep=2, sec/ep=4.56, ms/ts=2279.5, agent=100.0%](0.03+0j)\nEpisodes: 100%|██████████| 3/3 [00:17, reward=-0.12, ts/ep=2, sec/ep=4.50, ms/ts=2248.9, agent=100.0%]"
    }
   ],
   "source": [
    "# Proceed to training\n",
    "runner.run(\n",
    "    num_episodes=3,\n",
    "    save_best_agent=os.path.join(os.getcwd(), 'best_agent')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Episodes: 100%|██████████| 3/3 [00:17, reward=-0.12, ts/ep=2, sec/ep=4.50, ms/ts=2248.9, agent=100.0%]Learning finished. Total episodes: 3. Average reward of last 100 episodes: -0.11271708571155668.\n\n"
    }
   ],
   "source": [
    "import csv\n",
    "# Print statistics\n",
    "print(\"Learning finished. Total episodes: {ep}. Average reward of last 100 episodes: {ar}.\".format(\n",
    "    ep=runner.episodes,\n",
    "    ar=np.mean(runner.episode_rewards[-100:]))\n",
    ")\n",
    "\n",
    "name = \"returns_tf.csv\"\n",
    "if (not os.path.exists(\"saved_models\")):\n",
    "    os.mkdir(\"saved_models\")\n",
    "if (not os.path.exists(\"saved_models/\" + name)):\n",
    "    with open(\"saved_models/\" + name, \"w\") as csv_file:\n",
    "        spam_writer = csv.writer(csv_file, delimiter=\";\", lineterminator=\"\\n\")\n",
    "        spam_writer.writerow([\"Episode\", \"Return\"])\n",
    "        for ep in range(len(runner.episode_rewards)):\n",
    "            spam_writer.writerow([ep+1, runner.episode_rewards[ep]])\n",
    "\n",
    "runner.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitvenvvenv7f748138af854e409b8185cb62dad0fb",
   "display_name": "Python 3.7.4 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}